from fastai.vision.all import *
from fastbook import * 
import matplotlib.pyplot as plt
import webbrowser

# download a sample of mnist data (3s and 7s)
path = untar_data(URLs.MNIST_SAMPLE)
'''
print(path.ls())
print((path/'train').ls())
'''

# saves and sorts the image names, shows top 10 of the 3s
threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
'''
print(threes[:10])
'''

# visualization of an image as data
'''
# grab and show a data point for 3
im3_path = threes[1]
im3 = Image.open(im3_path)
im3.show()

# as numpy array
print(array(im3)[4:10,4:10]) # views rows from [4,10) and cols from [4,10)
# as pytorch tensor
print(tensor(im3)[4:10,4:10])
# use pandas dataframe to color code values using a gradient; shows how an image is conjured from pixel values
im3_t = tensor(im3)
df = pd.DataFrame(im3_t[4:26,4:22])
df_formatted = df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')
# open image generated by the dataframe in browser
with open('pixel_image.html', 'w') as image:
    df_formatted.to_html(image)
filename = 'pixel_image.html'
webbrowser.open_new_tab(filename)
'''

# first try - pixel similarity
seven_tensors = [tensor(Image.open(o)) for o in sevens]
three_tensors = [tensor(Image.open(o)) for o in threes]
'''
print(len(three_tensors),len(seven_tensors))
show_image(three_tensors[1], cmap='binary')
plt.show()
'''

# compute average of each pixel by stacking the tensors into a rank-3 tensor (3D tensor)
stacked_sevens = torch.stack(seven_tensors).float()/255
stacked_threes = torch.stack(three_tensors).float()/255

# investigating tensor jargon
'''
print(stacked_threes.shape) # 6131 28x28 images stacked as a 3D tensor
print(len(stacked_threes.shape))    # length of a tensor's shape = rank of the tensor ->
print(stacked_threes.ndim)          # rank of a tensor (ndim) = number of axes in a tensor 
'''

# compute the "ideal" 3 and 7
mean3 = stacked_threes.mean(0) 
mean7 = stacked_sevens.mean(0)
'''
show_image(mean3, cmap='binary')
plt.show()

show_image(mean7, cmap='binary')
plt.show()
'''

# sample 3 and 7
a_3 = stacked_threes[1]
a_7 = stacked_sevens[1]

# distance b/t a datapoint and the ideals
dist3_abs = (a_3 - mean3).abs().mean()
dist3_sqr = ((a_3 - mean3)**2).mean().sqrt()

dist7_abs = (a_3 - mean7).abs().mean()
dist7_sqr = ((a_3 - mean7)**2).mean().sqrt()

'''
print(dist3_abs, dist3_sqr)
print(dist7_abs, dist7_sqr)
print(F.l1_loss(a_3.float(), mean7), F.mse_loss(a_3, mean7).sqrt())
'''

# test pixel similarity method on validation set
valid_3_tens = torch.stack([tensor(Image.open(o))
                            for o in (path/'valid'/'3').ls()])
valid_3_tens = valid_3_tens.float()/255

valid_7_tens = torch.stack([tensor(Image.open(o))
                            for o in (path/'valid'/'7').ls()])
valid_7_tens = valid_3_tens.float()/255
'''
print(valid_3_tens.shape, valid_7_tens.shape)
'''

def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))
'''
print(mnist_distance(a_3, mean3))
'''

'''
valid_3_dist = mnist_distance(valid_3_tens, mean3)
print(valid_3_dist, valid_3_dist.shape)
'''

def is_3(x): return mnist_distance(x, mean3) < mnist_distance(x, mean7)

'''
print(is_3(a_3), is_3(a_3).float())
print(is_3(valid_3_tens))
'''

# calculate metrics
accuracy_3s = is_3(valid_3_tens).float().mean()
accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()
'''
print(accuracy_3s, accuracy_7s, (accuracy_3s+accuracy_7s)/2)
'''

# mnist loss function
# vectorize data and prepare labels for training set
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)
train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)
'''
print(tensor([1]*len(threes) + [0]*len(sevens)))
print(train_y)
print(train_x.shape, train_y.shape)
'''

# create a list of pairs; (image, label)
dset = list(zip(train_x, train_y))
x,y = dset[0]
'''
print(x.shape, y)
'''

# vectorize and prepare labels for validation set
valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)
valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x, valid_y))

# begin sgd

# init random parameters
def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()
weights = init_params((28*28, 1))
bias = init_params(1)

# predict (for one image)
'''
print((train_x[0]*weights.T).sum() + bias)
'''

# predictions for all of the training data
def linear1(xb): return xb@weights + bias
preds = linear1(train_x)
'''
print(preds)
'''

# check accuracy of predictions
corrects = (preds>0.0).float() == train_y
'''
print(corrects)
print(corrects.float().mean().item())
'''

# see effect of changing one weight
with torch.no_grad(): weights[0] *= 1.0001
preds = linear1(train_x)
'''
print(((preds>0.0).float() == train_y).float().mean().item())
'''

# first attempt loss function definition
trgts = tensor([1,0,1])
prds = tensor([0.9,0.4,0.2])

'''
def mnist_loss(predictions, targets):
    return torch.where(targets==1, 1-predictions, predictions).mean()

print(torch.where(trgts==1, 1-prds, prds))
print(mnist_loss(prds,trgts))
print(mnist_loss(tensor([0.9,0.4,0.8]),trgts))
'''

# sigmoid
def sigmoid(x): return 1/(1+torch.exp(-x))
plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)
'''
plt.show()
'''

# second attempt at loss function definition
def mnist_loss(predictions, targets):
    predictions = predictions.sigmoid()
    return torch.where(targets==1, 1-predictions, predictions).mean()

# mini-batch demo
coll = range(15)
dl = DataLoader(coll, batch_size=5, shuffle=True)
'''
print(list(dl))
'''
ds = L(enumerate(string.ascii_lowercase))
dl = DataLoader(ds, batch_size=6, shuffle=True)
'''
print(ds)
print(list(dl))
'''

###
# full implementation of gradient descent
